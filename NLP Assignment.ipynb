{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[Correct the Search Query](https://www.hackerrank.com/challenges/correct-the-search-query/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "who was the first president of india\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from difflib import get_close_matches\n",
    "import sys\n",
    "\n",
    "# Dictionary of valid words (for simplicity, using a small set)\n",
    "valid_words = set([\n",
    "    \"gong\", \"going\", \"to\", \"china\", \"who\", \"was\", \"the\", \"first\", \"president\",\n",
    "    \"of\", \"india\", \"winner\", \"match\", \"food\", \"in\", \"america\"\n",
    "    # Add more words as needed\n",
    "])\n",
    "\n",
    "# Function to get the closest valid word for a misspelled word\n",
    "def correct_word(word):\n",
    "    # Get the best match for the word from the valid words\n",
    "    candidates = get_close_matches(word, valid_words, n=1)\n",
    "    return candidates[0] if candidates else word\n",
    "\n",
    "# Function to split words in a query if needed\n",
    "def split_words(query):\n",
    "    # Try to split words by spaces and correct individual words\n",
    "    words = query.split()\n",
    "    corrected_words = [correct_word(word) for word in words]\n",
    "    return \" \".join(corrected_words)\n",
    "\n",
    "# Main function to handle the input/output\n",
    "def process_queries():\n",
    "    # Read the number of queries\n",
    "    num_queries = int(input().strip())\n",
    "    \n",
    "    for _ in range(num_queries):\n",
    "        query = input().strip()\n",
    "        # Process the query to correct the misspellings and segmentation issues\n",
    "        corrected_query = split_words(query)\n",
    "        print(corrected_query)\n",
    "\n",
    "# Call the main function to process the queries\n",
    "if __name__ == \"__main__\":\n",
    "    process_queries()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Deterministic Url and HashTag Segmentation](https://www.hackerrank.com/challenges/url-hashtag-segmentation/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letusgo\n"
     ]
    }
   ],
   "source": [
    "def load_lexicon():\n",
    "    # Simulated lexicon for illustration\n",
    "    return set([\n",
    "        \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"an\", \"and\", \"any\", \n",
    "        \"are\", \"as\", \"at\", \"back\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\",\n",
    "        \"between\", \"both\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"down\", \"during\", \"for\",\n",
    "        \"from\", \"go\", \"goes\", \"gone\", \"have\", \"having\", \"he\", \"he's\", \"here\", \"here's\", \n",
    "        \"his\", \"how\", \"how's\", \"i\", \"i'm\", \"i've\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\",\n",
    "        \"let\", \"like\", \"make\", \"man\", \"me\", \"my\", \"myself\", \"no\", \"nor\", \"not\", \"of\", \"off\",\n",
    "        \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"out\", \"over\", \"per\", \"re\", \"right\", \n",
    "        \"sought\", \"the\", \"theirs\", \"there\", \"there's\", \"they\", \"they're\", \"to\", \"under\", \n",
    "        \"until\", \"upon\", \"was\", \"we\", \"we're\", \"we've\", \"what\", \"what's\", \"when\", \"where\", \n",
    "        \"where's\", \"which\", \"who\", \"who's\", \"why\", \"why's\", \"with\", \"without\", \"you\", \"you're\", \n",
    "        \"you've\", \"your\", \"yourself\", \"yours\", \"down\", \"seconds\", \"earth\", \"hour\", \"human\",\n",
    "        \"time\", \"name\", \"my\", \"name\", \"namecheap\", \"big\", \"rock\", \"apple\", \"check\", \"domain\", \n",
    "        \"being\", \"follow\", \"back\", \"social\", \"media\", \"today\", \"let\", \"go\", \"this\", \"is\", \"insane\",\n",
    "        \"now\", \"to\", \"down\"\n",
    "    ])\n",
    "\n",
    "def segment_string(string, lexicon):\n",
    "    n = len(string)\n",
    "    dp = [None] * (n + 1)\n",
    "    dp[0] = []\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(i):\n",
    "            word = string[j:i]\n",
    "            if word in lexicon:\n",
    "                if dp[j] is not None:\n",
    "                    dp[i] = dp[j] + [word]\n",
    "                    break\n",
    "\n",
    "    return dp[n] if dp[n] is not None else [string]\n",
    "\n",
    "def preprocess_input(input_string):\n",
    "    # Remove 'www.' if it exists\n",
    "    if input_string.startswith('www.'):\n",
    "        input_string = input_string[4:]\n",
    "\n",
    "    # Remove domain extensions like .com, .org, .in\n",
    "    extensions = ['.com', '.org', '.net', '.edu', '.in', '.co', '.io']\n",
    "    for ext in extensions:\n",
    "        if input_string.endswith(ext):\n",
    "            input_string = input_string[:-len(ext)]\n",
    "            break\n",
    "\n",
    "    # Remove leading '#' for hashtags\n",
    "    if input_string.startswith('#'):\n",
    "        input_string = input_string[1:]\n",
    "\n",
    "    return input_string.lower()\n",
    "\n",
    "def segment_input(input_string, lexicon):\n",
    "    processed_input = preprocess_input(input_string)\n",
    "    segmented = segment_string(processed_input, lexicon)\n",
    "    return ' '.join(segmented)\n",
    "\n",
    "def main():\n",
    "    lexicon = load_lexicon()\n",
    "    \n",
    "    # Read the number of test cases\n",
    "    n = int(input().strip())\n",
    "    \n",
    "    for _ in range(n):\n",
    "        input_string = input().strip()\n",
    "        result = segment_input(input_string, lexicon)\n",
    "        print(result)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Disambiguation: Mouse vs Mouse](https://www.hackerrank.com/challenges/disambiguation-mouse-vs-mouse/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal\n"
     ]
    }
   ],
   "source": [
    "def classify_mouse(sentence):\n",
    "    # Convert the sentence to lowercase for easier comparison\n",
    "    sentence = sentence.lower()\n",
    "    \n",
    "    # Define keywords related to \"computer mouse\"\n",
    "    computer_mouse_keywords = ['input device', 'usb', 'click', 'pointer', 'cursor', 'computer']\n",
    "    \n",
    "    # Define keywords related to \"animal mouse\"\n",
    "    animal_mouse_keywords = ['genome', 'tail', 'postnatal', 'development', 'rodent']\n",
    "    \n",
    "    # Check for the presence of keywords for \"computer mouse\"\n",
    "    if any(keyword in sentence for keyword in computer_mouse_keywords):\n",
    "        return \"computer-mouse\"\n",
    "    \n",
    "    # Check for the presence of keywords for \"animal mouse\"\n",
    "    elif any(keyword in sentence for keyword in animal_mouse_keywords):\n",
    "        return \"animal\"\n",
    "    \n",
    "    # If no keywords match, we assume it's ambiguous; the problem doesn't specify this case.\n",
    "    # We will assume the sentence is referring to an \"animal\" mouse if no computer-related keywords are found.\n",
    "    else:\n",
    "        return \"animal\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Read the number of test cases\n",
    "    n = int(input().strip())\n",
    "    \n",
    "    # Process each sentence\n",
    "    for _ in range(n):\n",
    "        sentence = input().strip()\n",
    "        # Classify the sentence and print the result\n",
    "        print(classify_mouse(sentence))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Language Detection](https://www.hackerrank.com/challenges/language-detection/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import string\n",
    "\n",
    "# Gathered most common words from different languages and compared the input text to these words\n",
    "eng_words = 'the|of|and|to|a|in|for|is|on|that|by|this|with|i|you|it|not|or|be|are|from|at|as|your|all|have|new|more|an|was|we|will|home|can|us|about|if|page|my|has|search|free|but|our|one|other|do|no|information|time|they|site|he|up|may|what|which|their|news|out|use|any|there|see|only|so|his|when|contact|here|business|who|web|also|now|help|get|pm|view|online|c|e|first|am|been|would|how|were|me|s|services|some|these|click|its|like|service|x|than|find'\n",
    "spa_words = 'va|mientras|menos|momento|hacia|hace|estos|mayor|otro|antes|le|ver|dice|han|la|lo|vida|tu|vez|bien|otra|hay|decir|creo|te|porque|estaba|esa|yo|ya|cuando|nada|de|algunos|tanto|mucho|tambin|nos|ao|cosas|espa|desde|gran|sido|hoy|el|en|bueno|ser|otras|como|ejemplo|que|toda|as|sea|casi|todo|es|ademas|pues|nunca|muy|aqu|poco|ese|un|sus|estas|sobre|eso|vamos|solo|aos|tienen|forma|puede|segun|sino|les|que|como|aunque|veces|luego|tena|ahora|o|una|nosotros|habla|mismo|gente|uno|despues|por|durante|son|cada|donde|otros|tiene|siempre|m|contra|estan|pero|los|todas|ellos|poder|trabajo|a|ms|da|parte|personas|gobierno|ha|he|me|casa|caso|mi|fue|del|era|das|tres|usted|este|unos|esta|esto|al|mundo|general|pas|mejor|tal|mujer|tan|ni|para|no|parece|politica|hecho|pueden|s|sin|todos|algo|lugar|tiempo|est|ella|entonces|hombre|estado|las|hacer|e|entre|su|hasta|primera|si|y|dos|con|se'\n",
    "ger_words = 'august|siehe|kommt|etwa|begriff|immer|liste|selbst|meist|aber|weitere|als|denen|alle|auf|genannt|ihr|aus|einige|hatte|hat|ca|geschichte|waren|unter|beim|landkreis|de|da|band|isbn|das|leben|dr|bis|wenn|diesen|name|zeit|die|deutschland|teil|haben|erste|jedoch|ihn|kirche|bereits|kann|art|deutschen|jahrhundert|nur|welt|jahren|artikel|zu|es|er|wird|zwei|diesem|bekannt|werden|dieser|dieses|fr|gemeinde|dort|soll|menschen|welche|diese|ort|seine|auch|drei|nicht|ende|bezeichnung|je|sind|zur|wurden|of|jahre|literatur|ist|und|durch|zum|and|wie|einer|eines|namen|nach|keine|damit|eine|basisdaten|ihm|einem|usa|oder|liegt|befindet|was|war|sondern|konnte|viele|gegen|wurde|mnchen|adresse|gibt|beiden|heute|muss|schon|bei|karte|seit|januar|der|des|beispiel|um|dann|stadt|dem|den|politik|sein|ein|ihre|seinem|seinen|ab|wieder|ohne|noch|vom|von|dass|am|an|im|zwischen|vor|in|allem|ersten|mehr|seiner|verwendet|sowie|steht|form|bedeutung|bezeichnet|jahr|also|einwohner|sich|sie|neue|hier|verlag|anderen|mit|besteht|sehr|dabei|wappen|gilt|deutsche|man|bzw|deren|ihren|m|berlin|km|st|so|oft|the|ihrer'\n",
    "frn_words = 'vraiment|monde|l|comme|trop|femme|le|mais|la|donner|tu|ici|aux|te|regarder|ta|ami|me|de|personne|moi|ces|mon|ma|du|voir|sans|d|faire|toujours|vouloir|tres|l|partir|t|peutetre|attendre|oui|en|ses|tuer|laisser|chez|autre|et|jamais|homme|rien|quelque|peu|bien|sur|lui|avoir|accord|chose|fois|savoir|les|que|comprendre|dire|ser|qui|je|vrai|on|juste|oh|pouvoir|cette|s|tout|une|bon|estce|demander|ou|comment|aimer|mes|vie|croire|ce|son|besoin|passer|avec|parler|toi|penser|temps|venir|suivre|vous|arreter|sortir|meme|prendre|o|des|dans|pour|merci|un|falloir|mettre|connaitre|encore|aller|pere|petit|aussi|non|an|pourquoi|il|par|pas|quand|alors|seul|ne|mourir|deux|plus|quoi|ils|arriver|rester|devoir|notre|ca|elle|dieu|maintenant|jour|apres|mal|trouver|fille|si|y|nous|sa|se'\n",
    "\n",
    "en_count = 0\n",
    "spa_count = 0\n",
    "ger_count = 0\n",
    "frn_count = 0\n",
    "\n",
    "# Cleaning the string to get rid of punctuation marks and non-ASCII characters\n",
    "def cleanString(s):\n",
    "    # Remove punctuation\n",
    "    table = s.maketrans(\"\", \"\", string.punctuation)\n",
    "    cleaned = s.translate(table)\n",
    "    # Remove non-ASCII characters\n",
    "    return ''.join([c for c in cleaned if ord(c) < 128])\n",
    "\n",
    "# Taking multi-line input without knowing the number of input lines\n",
    "sentinel = ''  # ends when this string is seen\n",
    "line = input()  # Use input() instead of raw_input in Python 3\n",
    "for word in cleanString(line.strip()).split(' '):\n",
    "    if len(word) > 2:\n",
    "        if word in eng_words:\n",
    "            en_count += 1\n",
    "        elif word in spa_words:\n",
    "            spa_count += 1\n",
    "        elif word in ger_words:\n",
    "            ger_count += 1\n",
    "        elif word in frn_words:\n",
    "            frn_count += 1\n",
    "\n",
    "count_list = {'English': en_count, 'Spanish': spa_count, 'French': frn_count, 'German': ger_count}\n",
    "\n",
    "# Sort the count list by values in descending order\n",
    "sorted_count_list = sorted(count_list.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(sorted_count_list[0][0])  # This prints the language with the highest count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[The Missing Apostrophes](https://www.hackerrank.com/challenges/the-missing-apostrophes/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define common contractions for replacement\n",
    "contractions = {\n",
    "    \"dont\": \"don't\", \"didnt\": \"didn't\", \"cant\": \"can't\", \"isnt\": \"isn't\", \n",
    "    \"arent\": \"aren't\", \"wasnt\": \"wasn't\", \"werent\": \"weren't\", \"hasnt\": \"hasn't\",\n",
    "    \"havent\": \"haven't\", \"im\": \"I'm\", \"its\": \"it's\", \"youre\": \"you're\", \n",
    "    \"theyre\": \"they're\", \"whos\": \"who's\", \"whats\": \"what's\", \"id\": \"I'd\",\n",
    "    \"ive\": \"I've\", \"youve\": \"you've\", \"shes\": \"she's\", \"hes\": \"he's\", \n",
    "    \"were\": \"we're\", \"ill\": \"I'll\", \"youll\": \"you'll\", \"hell\": \"he'll\", \n",
    "    \"theyll\": \"they'll\"\n",
    "}\n",
    "\n",
    "def restore_apostrophes(text):\n",
    "    # Step 1: Handle common contractions\n",
    "    for wrong, correct in contractions.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    \n",
    "    # Step 2: Handle possessive nouns correctly\n",
    "    # Only add apostrophes to words that are possessive (e.g., 'Mark's') and not just plural nouns\n",
    "    text = re.sub(r\"(\\b\\w+'s\\b)(?=\\s)\", r\"\\1\", text)  # Keep apostrophe for possessives\n",
    "    \n",
    "    # Step 3: Fix contractions like 'hed' -> 'he'd', 'were' -> 'we're'\n",
    "    text = re.sub(r\"(\\bhe|she|they|we|I)(?=d\\b)\", r\"\\1'd\", text)  # Fix contraction forms\n",
    "    \n",
    "    return text\n",
    "\n",
    "def main():\n",
    "    # Read all the input at once (the paragraph might span multiple lines)\n",
    "    import sys\n",
    "    input_text = sys.stdin.read()\n",
    "    \n",
    "    # Call the function to restore apostrophes\n",
    "    fixed_text = restore_apostrophes(input_text)\n",
    "    \n",
    "    # Output the corrected text\n",
    "    print(fixed_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Segment the Twitter Hashtags](https://www.hackerrank.com/challenges/segment-the-twitter-hashtags/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are\n"
     ]
    }
   ],
   "source": [
    "def getDict():\n",
    "    # A sample word list to simulate the dictionary\n",
    "    words_arr = ['we', 'are', 'the', 'people', 'mention', 'your', 'faves', 'now', 'playing', 'the', 'walking', 'dead', 'follow', 'me']\n",
    "    word_dict = {}\n",
    "\n",
    "    for word in words_arr:\n",
    "        word_dict[word] = 0  # Add each word to the dictionary\n",
    "    return word_dict\n",
    "\n",
    "\n",
    "# Check if the word is present in the dictionary\n",
    "def isValidWord(word, word_dict):\n",
    "    return word in word_dict  # Return True if the word is in the dictionary\n",
    "\n",
    "\n",
    "# Splits the given string from left to right, checking for valid words\n",
    "# Prefer longer words first\n",
    "def getSegmentedWord(rword, word_dict):\n",
    "    start = 0\n",
    "    valid_words = []\n",
    "    while start < len(rword):\n",
    "        found = False\n",
    "        # Try all possible word lengths starting from the longest\n",
    "        for length in range(len(rword), start, -1):\n",
    "            if isValidWord(rword[start:length], word_dict):\n",
    "                valid_words.append(rword[start:length])\n",
    "                start = length\n",
    "                found = True\n",
    "                break\n",
    "        # If no valid word is found, move by one character\n",
    "        if not found:\n",
    "            start += 1\n",
    "    return valid_words\n",
    "\n",
    "\n",
    "# Main program starts here\n",
    "\n",
    "test_cases = int(input())  # Use input() instead of raw_input() in Python 3\n",
    "word_dict = getDict()  # Load the dictionary from the word list\n",
    "for _ in range(test_cases):\n",
    "    print(' '.join(getSegmentedWord(input().strip(), word_dict)))  # Process each test case and print the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Expand the Acronyms](https://www.hackerrank.com/challenges/expand-the-acronyms/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_acronyms_and_expansions(snippets):\n",
    "    acronym_expansion = {}\n",
    "    \n",
    "    # Process each snippet\n",
    "    for snippet in snippets:\n",
    "        # Match acronym-expansion pairs in the format: (ACRONYM) -> EXPANSION\n",
    "        matches = re.findall(r'([A-Z]{2,})\\s?\\(([^)]+)\\)', snippet)\n",
    "        \n",
    "        # Store the found acronyms and their expansions in the dictionary\n",
    "        for acronym, expansion in matches:\n",
    "            if acronym not in acronym_expansion:\n",
    "                acronym_expansion[acronym] = expansion\n",
    "    \n",
    "        # Handle cases like ACRONYM: EXPANSION format\n",
    "        matches = re.findall(r'([A-Z]{2,})\\s?:\\s?([^\\.]+)', snippet)\n",
    "        \n",
    "        # Store additional acronym-expansion pairs only if not already stored\n",
    "        for acronym, expansion in matches:\n",
    "            if acronym not in acronym_expansion:\n",
    "                acronym_expansion[acronym] = expansion\n",
    "            \n",
    "    return acronym_expansion\n",
    "\n",
    "def main():\n",
    "    N = int(input())  # Read the number of snippets\n",
    "    snippets = [input().strip() for _ in range(N)]  # Read all the snippets\n",
    "    \n",
    "    # Extract acronyms and their expansions from the snippets\n",
    "    acronym_expansion = extract_acronyms_and_expansions(snippets)\n",
    "    \n",
    "    # Process the test acronyms and output their expansions\n",
    "    for _ in range(N):\n",
    "        query = input().strip()  # Read the acronym to expand\n",
    "        if query in acronym_expansion:\n",
    "            print(acronym_expansion[query])  # Output the expansion for the given acronym\n",
    "\n",
    "# Execute the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A Text-Processing Warmup](https://www.hackerrank.com/challenges/a-text-processing-warmup/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1\n",
      "29\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def process_text_fragments(T, fragments):\n",
    "    # Define the regular expressions for articles and dates\n",
    "    article_patterns = {\n",
    "        'a': r'\\b a \\b',\n",
    "        'an': r'\\b an \\b',\n",
    "        'the': r'\\b the \\b'\n",
    "    }\n",
    "    \n",
    "    # Regex for detecting dates\n",
    "    date_pattern = r'(\\d{1,2})(st|nd|rd|th)? (\\d{1,2}) (January|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec|\\d{1,2}) (\\d{4})|'\\\n",
    "                   r'(\\d{1,2})\\/(\\d{1,2})\\/(\\d{4})'\n",
    "\n",
    "    for fragment in fragments:\n",
    "        # Count occurrences of articles\n",
    "        count_a = len(re.findall(article_patterns['a'], fragment))\n",
    "        count_an = len(re.findall(article_patterns['an'], fragment))\n",
    "        count_the = len(re.findall(article_patterns['the'], fragment))\n",
    "\n",
    "        # Count date occurrences\n",
    "        date_count = len(re.findall(date_pattern, fragment))\n",
    "        \n",
    "        # Output the results for this fragment\n",
    "        print(count_a)\n",
    "        print(count_an)\n",
    "        print(count_the)\n",
    "        print(date_count)\n",
    "\n",
    "# Input handling\n",
    "T = int(input())  # First line: number of test cases\n",
    "fragments = []\n",
    "\n",
    "# Reading input for T test cases\n",
    "for _ in range(T):\n",
    "    fragment = \"\"\n",
    "    while True:\n",
    "        try:\n",
    "            line = input()\n",
    "            if line == \"\":  # Blank line indicates the end of a fragment\n",
    "                break\n",
    "            fragment += line + \"\\n\"  # Keep accumulating the fragment\n",
    "        except EOFError:\n",
    "            break  # Handle unexpected end of input, if applicable\n",
    "    fragments.append(fragment.strip())  # Append the fragment after stripping the trailing newline\n",
    "\n",
    "# Call the function with the input provided\n",
    "process_text_fragments(T, fragments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Who is it?](https://www.hackerrank.com/challenges/who-is-it/problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def resolve_anaphora(text_lines, names_list):\n",
    "    # Step 1: Join the text into a single string for easier manipulation\n",
    "    text = \" \".join(text_lines)\n",
    "    \n",
    "    # Step 2: List of candidate entities/noun-phrases (split by ';')\n",
    "    candidates = names_list.split(';')\n",
    "    \n",
    "    # Step 3: Find all pronouns in the text (those surrounded by **)\n",
    "    pronouns = []\n",
    "    pronoun_pattern = re.compile(r'\\*\\*([a-zA-Z]+)\\*\\*')\n",
    "    \n",
    "    # Gather all pronouns with their start and end positions\n",
    "    for match in pronoun_pattern.finditer(text):\n",
    "        pronouns.append((match.group(1), match.start(), match.end()))\n",
    "    \n",
    "    # Step 4: Initialize a list to store results\n",
    "    results = []\n",
    "    \n",
    "    # Step 5: Resolve each pronoun\n",
    "    for pronoun, start, end in pronouns:\n",
    "        # We need to find the most recent noun or noun-phrase that appears before this pronoun\n",
    "        candidate_match = None\n",
    "        for candidate in candidates:\n",
    "            # Search for the candidate in the text before the pronoun position\n",
    "            candidate_pos = text.lower().find(candidate.lower(), 0, start)\n",
    "            if candidate_pos != -1:\n",
    "                candidate_match = candidate\n",
    "        \n",
    "        # Add the matched candidate to the results\n",
    "        if candidate_match:\n",
    "            results.append(candidate_match)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Read input\n",
    "N = int(input())  # Number of lines in the text\n",
    "text_lines = [input().strip() for _ in range(N)]  # The text itself (next N lines)\n",
    "names_list = input().strip()  # List of candidate names/noun-phrases\n",
    "\n",
    "# Resolve the pronouns in the text\n",
    "result = resolve_anaphora(text_lines, names_list)\n",
    "\n",
    "# Output the results, each result on a new line\n",
    "for r in result:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
